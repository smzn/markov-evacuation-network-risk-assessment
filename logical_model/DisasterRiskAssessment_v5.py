import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from dataclasses import dataclass
import networkx as nx
from scipy.interpolate import RegularGridInterpolator
import matplotlib.cm as cm
import matplotlib.colors as mcolors
import matplotlib.patches as mpatches
from sklearn.metrics import pairwise_distances
import matplotlib.patches as patches
import os

@dataclass
class Location:
    id: int
    type: str  # 'person' or 'shelter'
    x: float
    y: float
    z: float

class DisasterRiskAssessment:
    def __init__(self, width=10000, height=10000,
                 num_people=300, num_shelters=30,
                 mean_z=15, std_z=5, max_z=100):
        """
        ç½å®³ãƒªã‚¹ã‚¯æŒ‡æ¨™è¨ˆç®—ã‚¯ãƒ©ã‚¹
        """
        self.width = width
        self.height = height
        self.num_people = num_people
        self.num_shelters = num_shelters
        self.mean_z = mean_z
        self.std_z = std_z
        self.max_z = max_z
        self.people = None
        self.shelters = None

    def generate_terrain(self, resolution=300,
                        num_peaks=6, mean_height=80, std_height=30,
                        min_spread=400, max_spread=1200, seed=None):
        """
        å¤šé‡ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«ã‚ˆã‚‹æ¨™é«˜ãƒãƒƒãƒ—ã‚’ç”Ÿæˆï¼ˆterrain_xx, yy, zzã‚’ä¿å­˜ï¼‰
        """
        if seed is not None:
            np.random.seed(seed)

        x = np.linspace(0, self.width, resolution)
        y = np.linspace(0, self.height, resolution)
        xx, yy = np.meshgrid(x, y)
        zz = np.zeros_like(xx)

        for _ in range(num_peaks):
            cx = np.random.uniform(0, self.width)
            cy = np.random.uniform(0, self.height)
            h = max(0, np.random.normal(mean_height, std_height))
            s = np.random.uniform(min_spread, max_spread)
            zz += h * np.exp(-((xx - cx)**2 + (yy - cy)**2) / (2 * s**2))

        self.terrain_xx = xx
        self.terrain_yy = yy
        self.terrain_zz = zz

    def get_elevation_at(self, x, y):
        """
        åœ°å½¢ãƒãƒƒãƒ—ã‹ã‚‰ä»»æ„åº§æ¨™ (x, y) ã®æ¨™é«˜ã‚’è£œé–“å–å¾—
        """
        if not hasattr(self, "terrain_zz"):
            raise RuntimeError("åœ°å½¢ãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚generate_terrain() ã‚’å…ˆã«å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚")

        xi = np.linspace(0, self.width, self.terrain_xx.shape[1])
        yi = np.linspace(0, self.height, self.terrain_yy.shape[0])
        interpolator = RegularGridInterpolator((yi, xi), self.terrain_zz)

        point = np.array([[y, x]])  # æ³¨æ„ï¼šscipyã¯ (y, x) é †
        return float(interpolator(point))
    
    def generate_locations_from_terrain(self):
        """
        åœ°å½¢ã«åŸºã¥ã„ã¦é¿é›£å¯¾è±¡è€…ã¨é¿é›£æ‰€ã®æ‹ ç‚¹ã‚’ç”Ÿæˆ
        """
        if not hasattr(self, "terrain_zz"):
            raise RuntimeError("åœ°å½¢ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã« generate_terrain() ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚")

        self.people = self._generate_locations(self.num_people, "person", start_id=0)
        self.shelters = self._generate_locations(self.num_shelters, "shelter", start_id=self.num_people)


    def save_terrain_3d_plot(self, filename="terrain_3d_map.png"):
        """
        terrain_zz ã‚’ 3Dã‚µãƒ¼ãƒ•ã‚§ã‚¹ã§PNGä¿å­˜
        """
        if not hasattr(self, "terrain_zz"):
            raise RuntimeError("åœ°å½¢ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã« generate_terrain() ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚")

        fig = plt.figure(figsize=(10, 8))
        ax = fig.add_subplot(111, projection='3d')
        ax.plot_surface(self.terrain_xx, self.terrain_yy, self.terrain_zz,
                        cmap='terrain', linewidth=0, antialiased=False)
        ax.set_title("Generated Terrain (3D View)")
        ax.set_xlabel("X [m]")
        ax.set_ylabel("Y [m]")
        ax.set_zlabel("Elevation [m]")

        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Saved terrain 3D map to {filename}")


    def _generate_locations(self, n, loc_type, start_id=0):
        """
        ãƒ©ãƒ³ãƒ€ãƒ ãªä½ç½®ã¨æ¨™é«˜ã‚’æŒã¤Locationãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
        """
        locations = []
        for i in range(n):
            x = np.random.uniform(0, self.width)
            y = np.random.uniform(0, self.height)
            z = self.get_elevation_at(x, y)  # åœ°å½¢ã‹ã‚‰å–å¾—
            #z = np.clip(np.random.normal(self.mean_z, self.std_z), 0, self.max_z)
            locations.append(Location(id=start_id + i, type=loc_type, x=x, y=y, z=z))
        return locations

    def _generate_elevation_grid(self, resolution=100):
        """
        ã‚°ãƒªãƒƒãƒ‰çŠ¶ã®æ¨™é«˜ï¼ˆæ­£è¦åˆ†å¸ƒï¼‰ã‚’ç”Ÿæˆã—ã¦ç­‰é«˜ç·šç”¨ã«ä½¿ç”¨
        """
        x = np.linspace(0, self.width, resolution)
        y = np.linspace(0, self.height, resolution)
        xx, yy = np.meshgrid(x, y)
        zz = np.random.normal(self.mean_z, self.std_z, size=xx.shape)
        zz = np.clip(zz, 0, 300)
        return xx, yy, zz

    def show_area(self, filename="evacuation_map.png", figsize=(5, 5), dpi=200):
        """
        æ‹ ç‚¹ã¨ç­‰é«˜ç·šã‚’è¡¨ç¤º
        """
        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)

        print(f"Figure size (inch): {fig.get_size_inches()}")
        print(f"DPI: {fig.get_dpi()}")
        print(f"Figure size (px): {fig.get_size_inches() * fig.get_dpi()}")

        ax.set_xlim(0, self.width)
        ax.set_ylim(0, self.height)
        ax.set_aspect('equal')
        ax.set_title(f"Evacuation Map ({self.num_people} people, {self.num_shelters} shelters)")
        ax.set_xlabel("X [m]")
        ax.set_ylabel("Y [m]")
        ax.grid(True)

        # ç­‰é«˜ç·šæç”»
        xx, yy, zz = self._generate_elevation_grid()
        contour = ax.contourf(xx, yy, zz, levels=20, cmap="terrain", alpha=0.4)

        # æ‹ ç‚¹æç”»
        px = [p.x for p in self.people]
        py = [p.y for p in self.people]
        ax.scatter(px, py, color='red', s=10, label="People")

        sx = [s.x for s in self.shelters]
        sy = [s.y for s in self.shelters]
        ax.scatter(sx, sy, color='blue', s=30, marker='^', label="Shelters")

        ax.legend(loc='center left', bbox_to_anchor=(-0.1, -0.1), borderaxespad=0.)
        plt.colorbar(contour, ax=ax, label="Elevation [m]")
        #plt.show()
        # ä¿å­˜ï¼ˆè¡¨ç¤ºã—ãªã„ï¼‰
        plt.savefig(filename, bbox_inches='tight')
        plt.close()
        print(f"Saved area plot to {filename}")

    def save_to_csv(self, filename="locations.csv", person_popularity=1, shelter_popularity=30):
        """
        æ‹ ç‚¹æƒ…å ±ã‚’CSVã«ä¿å­˜ã—ã€äººæ°—åº¦åˆ—ã‚’è¿½åŠ 
        """
        all_locations = self.people + self.shelters
        df = pd.DataFrame([vars(loc) for loc in all_locations])

        # popularityåˆ—ã®è¿½åŠ 
        df["popularity"] = df["type"].apply(lambda t: person_popularity if t == "person" else shelter_popularity)

        df.to_csv(filename, index=False)
        print(f"Saved with popularity: {filename}")


    def compute_distance_matrix(self, locations):
        """
        æ‹ ç‚¹é–“ã®ç›´ç·šè·é›¢è¡Œåˆ—ï¼ˆ2æ¬¡å…ƒï¼‰ã‚’è¿”ã™
        """
        n = len(locations)
        data = np.zeros((n, n))

        for i in range(n):
            for j in range(n):
                dx = locations[i].x - locations[j].x
                dy = locations[i].y - locations[j].y
                distance = np.sqrt(dx**2 + dy**2)
                data[i, j] = distance

        index = [loc.id for loc in locations]
        df = pd.DataFrame(data, index=index, columns=index)
        return df
    
    def load_locations_from_csv(self, filename="locations.csv"):
        df = pd.read_csv(filename)
        locations = []
        for _, row in df.iterrows():
            loc = Location(id=int(row["id"]),
                        type=row["type"],
                        x=row["x"],
                        y=row["y"],
                        z=row["z"])
            locations.append(loc)
        return locations, df.set_index("id")

    def compute_lowland_fraction_matrix(self,
                                        rm=10.0,
                                        n_samples=50,
                                        output_csv="lowland_fraction_matrix.csv"):
        """
        ã™ã¹ã¦ã®æ‹ ç‚¹é–“ã§ã€ç›´ç·šä¸Šã«ãŠã‘ã‚‹æ¨™é«˜rmä»¥ä¸‹ã®å‰²åˆã‚’ç®—å‡ºã—ã€è¡Œåˆ—ã¨ã—ã¦CSVã«ä¿å­˜ã™ã‚‹
        """
        # åœ°å½¢ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        if not hasattr(self, "terrain_zz"):
            raise RuntimeError("åœ°å½¢ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚generate_terrain() ã‚’å…ˆã«å‘¼ã‚“ã§ãã ã•ã„ã€‚")

        # æ‹ ç‚¹ãƒ‡ãƒ¼ã‚¿çµåˆ
        all_locations = self.people + self.shelters
        df = pd.DataFrame([vars(loc) for loc in all_locations])
        df = df.sort_values("id").reset_index(drop=True)
        ids = df["id"].tolist()
        coords = df[["x", "y"]].to_numpy()
        n = len(ids)

        # æ¨™é«˜è£œé–“å™¨ã®æº–å‚™
        xi = np.linspace(0, self.width, self.terrain_xx.shape[1])
        yi = np.linspace(0, self.height, self.terrain_yy.shape[0])
        interpolator = RegularGridInterpolator((yi, xi), self.terrain_zz)

        result = np.zeros((n, n))

        for i in range(n):
            for j in range(n):
                if i == j:
                    continue  # è‡ªå·±é·ç§»ã¯ã‚¹ã‚­ãƒƒãƒ—
                x0, y0 = coords[i]
                x1, y1 = coords[j]
                xs = np.linspace(x0, x1, n_samples)
                ys = np.linspace(y0, y1, n_samples)
                points = np.vstack((ys, xs)).T  # (y, x) é †

                try:
                    zs = interpolator(points)
                except ValueError:
                    zs = np.full(n_samples, 9999)  # ç¯„å›²å¤–ãªã‚‰é«˜åœ°æ‰±ã„

                ratio = np.mean(zs <= rm)
                result[i, j] = ratio

        df_result = pd.DataFrame(result, index=ids, columns=ids)
        df_result.to_csv(output_csv)
        print(f"Saved: {output_csv}")
        return df_result
    
    def compute_transition_matrix_from_csv(self, location_csv="locations.csv",
                                        alpha=1.0, beta=1.0, gamma=1.0,
                                        epsilon=1e-3,
                                        output_csv="transition_matrix.csv",
                                        lowland_matrix_csv=None, rp=None):
        """
        popularityåˆ—ã‚’å«ã‚€locations.csvã‚’èª­ã¿è¾¼ã¿ã€é‡åŠ›ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ãæ¨ç§»ç¢ºç‡è¡Œåˆ—ã‚’è¨ˆç®—ãƒ»ä¿å­˜ã€‚
        ä»»æ„ã§ã€ä½åœ°å‰²åˆè¡Œåˆ—ã‚’ç”¨ã„ã¦å±é™ºçµŒè·¯ã‚’é™¤å¤–ã€‚
        """
        # æ‹ ç‚¹æƒ…å ±èª­ã¿è¾¼ã¿
        df = pd.read_csv(location_csv)
        if "popularity" not in df.columns:
            raise ValueError("locations.csv ã« 'popularity' åˆ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        df = df.sort_values("id").reset_index(drop=True)
        ids = df["id"].tolist()
        coords = df[["x", "y"]].to_numpy()

        # è·é›¢è¡Œåˆ—
        dist_matrix = np.linalg.norm(coords[:, None, :] - coords[None, :, :], axis=2) + epsilon
        np.fill_diagonal(dist_matrix, np.inf)

        # äººæ°—åº¦ãƒ™ã‚¯ãƒˆãƒ«
        p = df["popularity"].to_numpy()

        # é‡åŠ›ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬é‡ã¿è¡Œåˆ—
        weight_matrix = (p[:, None] ** alpha) * (p[None, :] ** beta) / (dist_matrix ** gamma)

        # ä½åœ°å‰²åˆè¡Œåˆ—ã®ãƒ•ã‚£ãƒ«ã‚¿å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if lowland_matrix_csv and rp is not None:
            lowland_df = pd.read_csv(lowland_matrix_csv, index_col=0)
            lowland_df.index = lowland_df.index.astype(int)
            lowland_df.columns = lowland_df.columns.astype(int)
            lowland_arr = lowland_df.loc[ids, ids].to_numpy()
            # rpä»¥ä¸Šã®çµŒè·¯ã¯ä½¿ãˆãªã„ï¼weightã‚’0ã«ã™ã‚‹
            weight_matrix[lowland_arr >= rp] = 0.0

        # æ­£è¦åŒ–ï¼ˆè¡Œã”ã¨ã«ç¢ºç‡åŒ–ï¼‰
        row_sums = weight_matrix.sum(axis=1, keepdims=True)
        transition_matrix = np.divide(weight_matrix, row_sums, where=row_sums != 0)

        # ä¿å­˜
        df_result = pd.DataFrame(transition_matrix, index=ids, columns=ids)
        df_result.to_csv(output_csv)
        print(f"Saved: {output_csv}")
        return df_result

    def analyze_equivalence_classes(self,
                                    transition_matrix_csv="transition_matrix.csv",
                                    location_csv="locations.csv",
                                    output_node_csv="equivalence_assignment.csv",
                                    output_summary_csv="equivalence_summary.csv",
                                    threshold=1e-4):
        """
        æ¨ç§»ç¢ºç‡è¡Œåˆ—ã«åŸºã¥ã„ã¦åŒå€¤é¡ã”ã¨ã«å®šå¸¸åˆ†å¸ƒãƒ»ä¸­å¿ƒæ€§ã‚’è¨ˆç®—ã—ã€æ­£è¦åŒ–ã—ãŸä¸­å¿ƒæ€§ã‚‚å«ã‚ã¦CSVå‡ºåŠ›
        """
        # å…¥åŠ›èª­ã¿è¾¼ã¿
        P = pd.read_csv(transition_matrix_csv, index_col=0)
        P.index = P.index.astype(int)
        P.columns = P.columns.astype(int)
        df_loc = pd.read_csv(location_csv).set_index("id")

        # æœ‰å‘ã‚°ãƒ©ãƒ• G
        G = nx.DiGraph()
        for i in P.index:
            for j in P.columns:
                if P.at[i, j] >= threshold:
                    G.add_edge(i, j, weight=P.at[i, j])

        # ç„¡å‘ã‚°ãƒ©ãƒ•ï¼ˆä¸­å¿ƒæ€§ç”¨ï¼‰
        P_undir = (P + P.T) / 2
        G_undir = nx.Graph()
        for i in P.index:
            for j in P.columns:
                if P_undir.at[i, j] >= threshold:
                    G_undir.add_edge(i, j, weight=P_undir.at[i, j])

        # åŒå€¤é¡ï¼ˆå¼·é€£çµæˆåˆ†ï¼‰
        components = list(nx.strongly_connected_components(G))
        assignment_rows = []
        summary_rows = []

        for idx, comp in enumerate(components):
            eq_class = f"E{idx}"
            nodes = sorted(comp)
            subG = G.subgraph(nodes)
            subG_undir = G_undir.subgraph(nodes)

            # å®šå¸¸åˆ†å¸ƒï¼ˆPageRankã§è¿‘ä¼¼ï¼‰
            try:
                pi = nx.pagerank(subG, weight='weight')
            except:
                pi = {n: 1 / len(nodes) for n in nodes}

            # ä¸­å¿ƒæ€§ï¼ˆéæ­£è¦åŒ–ï¼‰
            in_deg = dict(subG.in_degree(weight=None))
            out_deg = dict(subG.out_degree(weight=None))
            undeg = dict(subG_undir.degree(weight=None))

            # æ­£è¦åŒ–ç”¨ã®æœ€å¤§å€¤å–å¾—
            max_in = max(in_deg.values()) if in_deg else 0
            max_out = max(out_deg.values()) if out_deg else 0
            max_undeg = max(undeg.values()) if undeg else 0

            for n in nodes:
                row = {
                    "id": n,
                    "type": df_loc.at[n, "type"],
                    "x": df_loc.at[n, "x"],
                    "y": df_loc.at[n, "y"],
                    "z": df_loc.at[n, "z"],
                    "equivalence_class": eq_class,
                    "stationary": pi.get(n, 0),
                    "in_degree": in_deg.get(n, 0),
                    "out_degree": out_deg.get(n, 0),
                    "undirected_degree": undeg.get(n, 0),
                    "in_degree_norm": in_deg[n] / max_in if max_in > 0 else 0,
                    "out_degree_norm": out_deg[n] / max_out if max_out > 0 else 0,
                    "undirected_degree_norm": undeg[n] / max_undeg if max_undeg > 0 else 0
                }
                assignment_rows.append(row)

            summary_rows.append({
                "equivalence_class": eq_class,
                "num_total": len(nodes),
                "num_person": sum(df_loc.loc[n, "type"] == "person" for n in nodes),
                "num_shelter": sum(df_loc.loc[n, "type"] == "shelter" for n in nodes),
                "member_ids": str(nodes),
            })

        # ä¿å­˜
        pd.DataFrame(assignment_rows).to_csv(output_node_csv, index=False)
        pd.DataFrame(summary_rows).to_csv(output_summary_csv, index=False)
        print(f"Saved: {output_node_csv}, {output_summary_csv}")


    def visualize_equivalence_classes(self,
                                    eq_csv_path="equivalence_assignment.csv",
                                    output_path="equivalence_map.png",
                                    figsize=(10, 10), dpi=300):
        """
        åŒå€¤é¡ã®åˆ†é¡ã‚’åœ°å½¢ä¸Šã«å¯è¦–åŒ–ã—ã€PNGã§ä¿å­˜
        - åŒå€¤é¡ã‚’è‰²åˆ†ã‘ã€æœ€å¤§ã‚¯ãƒ©ã‚¹ä»¥å¤–ã‚’è–„ã
        - personã¨shelterã‚’ã‚¢ã‚¤ã‚³ãƒ³ã§åŒºåˆ¥
        - ç­‰é«˜ç·šã‚’è¡¨ç¤ºï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§ã¯ãªã„ï¼‰
        - IDã‚’æ‹ ç‚¹ã«ãƒ©ãƒ™ãƒ«è¡¨ç¤º
        """
        if not hasattr(self, "terrain_zz"):
            raise RuntimeError("åœ°å½¢ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚generate_terrain() ã‚’å…ˆã«å‘¼ã‚“ã§ãã ã•ã„ã€‚")

        eq_df = pd.read_csv(eq_csv_path)
        unique_classes = sorted(eq_df["equivalence_class"].unique())
        class_colors = cm.get_cmap('tab20', len(unique_classes))
        color_map = {cls: class_colors(i) for i, cls in enumerate(unique_classes)}

        # æœ€å¤§åŒå€¤é¡ã‚’ç‰¹å®š
        class_sizes = eq_df["equivalence_class"].value_counts()
        largest_class = class_sizes.idxmax()

        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)

        # ç­‰é«˜ç·šï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§ãªãç·šï¼‰
        ax.contour(self.terrain_xx, self.terrain_yy, self.terrain_zz,
                levels=20, cmap="terrain", linewidths=0.7)

        # æ‹ ç‚¹ãƒ—ãƒ­ãƒƒãƒˆ
        for _, row in eq_df.iterrows():
            x, y, node_id = row["x"], row["y"], int(row["id"])
            cls, t = row["equivalence_class"], row["type"]
            color = color_map[cls]
            in_main_class = (cls == largest_class)
            alpha = 1.0 if in_main_class else 0.6

            # ãƒãƒ¼ã‚«ãƒ¼å½¢çŠ¶ã‚’åˆ†é¡
            if t == "person":
                marker = 'o' if in_main_class else '*'
                size = 30 if in_main_class else 60
            else:  # shelter
                marker = '^' if in_main_class else 'D'
                size = 70 if in_main_class else 60

            ax.scatter(x, y, marker=marker, s=size, color=color,
                       edgecolors='black', alpha=alpha)
            ax.text(x + 20, y + 20, str(node_id), fontsize=6, color='black', alpha=alpha)

        legend_elements = [
            plt.Line2D([0], [0], marker='o', color='w', label='Person (Main Class)',
                    markerfacecolor='gray', markersize=6, markeredgecolor='black'),
            plt.Line2D([0], [0], marker='*', color='w', label='Person (Isolated)',
                    markerfacecolor='gray', markersize=8, markeredgecolor='black'),
            plt.Line2D([0], [0], marker='^', color='w', label='Shelter (Main Class)',
                    markerfacecolor='gray', markersize=8, markeredgecolor='black'),
            plt.Line2D([0], [0], marker='D', color='w', label='Shelter (Isolated)',
                    markerfacecolor='gray', markersize=7, markeredgecolor='black')
        ]
        ax.legend(handles=legend_elements, loc='upper right', fontsize=8)

        ax.set_title("Equivalence Class Map with Terrain", fontsize=12)
        ax.set_xlabel("X [m]")
        ax.set_ylabel("Y [m]")
        ax.set_aspect('equal')
        ax.grid(True)
        plt.tight_layout()

        plt.savefig(output_path, bbox_inches='tight')
        plt.close()
        print(f"Saved: {output_path}")
        

    def compute_disk_cover(self, R=1000, grid_step=100,
                        input_node_csv="equivalence_assignment.csv",
                        output_csv="disk_cover_result.csv"):
        """
        æœ€å¤§åŠå¾„Rã®å††ã§ã€ä¸»åŒå€¤é¡ä»¥å¤–ã®ãƒãƒ¼ãƒ‰ã‚’ã‚°ãƒªãƒƒãƒ‰ä¸Šã®å††ä¸­å¿ƒã§è²ªæ¬²ã«è¢«è¦†ã™ã‚‹
        - å††ã®ä¸­å¿ƒå€™è£œã¯ grid_step[m] é–“éš”ã®ã‚°ãƒªãƒƒãƒ‰ç‚¹
        - è¢«è¦†æˆåŠŸã”ã¨ã« covered ã‚»ãƒƒãƒˆã‚’æ›´æ–°
        - æœ€çµ‚çš„ã«è¢«è¦†å††ã®ä¸­å¿ƒãƒ»åŠå¾„ãƒ»ã‚«ãƒãƒ¼å¯¾è±¡IDã‚’ CSV ã«ä¿å­˜
        """

        # 1. æ‹ ç‚¹èª­ã¿è¾¼ã¿
        df = pd.read_csv(input_node_csv)
        main_class = df["equivalence_class"].value_counts().idxmax()
        df_target = df[df["equivalence_class"] != main_class].copy()
        target_coords = df_target[["x", "y"]].to_numpy()
        target_ids = df_target["id"].tolist()

        if len(target_coords) == 0:
            print("å¯¾è±¡ãƒãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…¨ã¦ä¸»åŒå€¤é¡ã«å±ã—ã¦ã„ã¾ã™ã€‚")
            return None

        print(f"å¯¾è±¡ãƒãƒ¼ãƒ‰æ•°: {len(target_coords)}")
        print(f"å¯¾è±¡IDä¸€è¦§: {target_ids}")

        # 2. ã‚°ãƒªãƒƒãƒ‰ä¸­å¿ƒå€™è£œä½œæˆ
        grid_x = np.arange(0, self.width + grid_step, grid_step)
        grid_y = np.arange(0, self.height + grid_step, grid_step)
        grid_points = np.array([[x, y] for x in grid_x for y in grid_y])

        # 3. å„ä¸­å¿ƒã‹ã‚‰Rä»¥å†…ã«ã‚ã‚‹ãƒãƒ¼ãƒ‰ã‚’æ¢ç´¢
        dists = pairwise_distances(grid_points, target_coords)
        cover_matrix = dists <= R  # [grid_point_index, target_index]

        covered = set()
        result_rows = []
        iteration = 0
        total = len(target_ids)

        print(f"Starting disk cover computation (R={R}, grid_step={grid_step})...")

        # 4. è²ªæ¬²ã«æœ€å¤§ã‚«ãƒãƒ¼ã™ã‚‹ä¸­å¿ƒã‚’é¸ã³ç¶šã‘ã‚‹
        while len(covered) < total:
            # æœªã‚«ãƒãƒ¼å¯¾è±¡ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            uncovered_mask = np.array([tid not in covered for tid in target_ids])
            effective_covers = (cover_matrix[:, uncovered_mask]).sum(axis=1)

            best_idx = np.argmax(effective_covers)
            max_cover = effective_covers[best_idx]

            if max_cover == 0:
                print("ã™ã§ã«ã‚«ãƒãƒ¼ã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã—ã‹è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚çµ‚äº†ã—ã¾ã™ã€‚")
                break

            # å®Ÿéš›ã«ã“ã®ä¸­å¿ƒã§ã‚«ãƒãƒ¼ã§ãã‚‹ ID ã‚’å–å¾—
            cover_idxs = np.where(cover_matrix[best_idx])[0]
            cover_ids = [target_ids[i] for i in cover_idxs if target_ids[i] not in covered]
            for cid in cover_ids:
                covered.add(cid)

            result_rows.append({
                "center_x": grid_points[best_idx, 0],
                "center_y": grid_points[best_idx, 1],
                "radius": R,
                "covered_ids": str(cover_ids),
                "num_covered": len(cover_ids)
            })

            iteration += 1
            print(f"[Iteration {iteration}] Covered: {len(covered)} / {total} ({len(covered)/total:.1%})")

        # çµæœä¿å­˜
        result_df = pd.DataFrame(result_rows)
        result_df.to_csv(output_csv, index=False)
        print(f"âœ… Disk cover completed: {iteration} disks used.")
        print(f"Saved result to: {output_csv}")

        return output_csv

    def visualize_equivalence_and_disks(self,
                                        eq_csv_path="equivalence_assignment.csv",
                                        disk_csv_path="disk_cover_result.csv",
                                        output_path="equivalence_with_disks.png",
                                        rm=None,
                                        figsize=(10, 10), dpi=300):
        """
        åŒå€¤é¡ã®åˆ†é¡ã¨è¢«è¦†å††ã‚’åœ°å½¢ä¸Šã«å¯è¦–åŒ–ã—ã¦ä¿å­˜
        - åŒå€¤é¡ã‚’è‰²åˆ†ã‘ã€æœ€å¤§ã‚¯ãƒ©ã‚¹ä»¥å¤–ã‚’åŒºåˆ¥
        - person/shelterã‚’å½¢çŠ¶ã§åŒºåˆ¥
        - æœ€å¤§åŒå€¤é¡å¤–: personâ†’æ˜Ÿã€shelterâ†’ã²ã—å½¢
        - è¢«è¦†å††ã‚’èµ¤ã®ç ´ç·šã§è¡¨ç¤º
        - ç­‰é«˜ç·šã‚’èƒŒæ™¯ã«è¡¨ç¤º
        """
        if not hasattr(self, "terrain_zz"):
            raise RuntimeError("åœ°å½¢ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚generate_terrain() ã‚’å…ˆã«å‘¼ã‚“ã§ãã ã•ã„ã€‚")

        eq_df = pd.read_csv(eq_csv_path)
        disk_df = pd.read_csv(disk_csv_path)
        unique_classes = sorted(eq_df["equivalence_class"].unique())
        class_colors = cm.get_cmap('tab20', len(unique_classes))
        color_map = {cls: class_colors(i) for i, cls in enumerate(unique_classes)}

        class_sizes = eq_df["equivalence_class"].value_counts()
        largest_class = class_sizes.idxmax()

        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)

        if rm is not None:
            rm_str = f"_ristricted{int(rm)}"
            title_suffix = f" (Restricted Elevation â‰¤ {int(rm)}m)"
        else:
            rm_str = ""
            title_suffix = ""

        output_path = f"{output_path}{rm_str}.png"


        # ç­‰é«˜ç·šè¡¨ç¤º
        ax.contour(self.terrain_xx, self.terrain_yy, self.terrain_zz,
                levels=20, cmap="terrain", linewidths=0.7)

        # æ‹ ç‚¹ãƒ—ãƒ­ãƒƒãƒˆ
        for _, row in eq_df.iterrows():
            x, y, node_id = row["x"], row["y"], int(row["id"])
            cls, t = row["equivalence_class"], row["type"]
            color = color_map[cls]
            in_main_class = (cls == largest_class)
            alpha = 1.0 if in_main_class else 0.7

            # ãƒãƒ¼ã‚«ãƒ¼å½¢çŠ¶
            if t == "person":
                marker = 'o' if in_main_class else '*'
                size = 30 if in_main_class else 60
            else:
                marker = '^' if in_main_class else 'D'
                size = 70 if in_main_class else 60

            ax.scatter(x, y, marker=marker, s=size, color=color,
                    edgecolors='black', alpha=alpha)
            ax.text(x + 20, y + 20, str(node_id), fontsize=6, color='black', alpha=alpha)

        # è¢«è¦†å††ã®æç”»
        for _, row in disk_df.iterrows():
            cx, cy, radius = row["center_x"], row["center_y"], row["radius"]
            circle = plt.Circle((cx, cy), radius, color='red', fill=False,
                                linestyle='--', linewidth=1.5, alpha=0.7)
            ax.add_patch(circle)

        # å‡¡ä¾‹
        legend_elements = [
            plt.Line2D([0], [0], marker='o', color='w', label='Person (Main Class)',
                    markerfacecolor='gray', markersize=6, markeredgecolor='black'),
            plt.Line2D([0], [0], marker='*', color='w', label='Person (Isolated)',
                    markerfacecolor='gray', markersize=8, markeredgecolor='black'),
            plt.Line2D([0], [0], marker='^', color='w', label='Shelter (Main Class)',
                    markerfacecolor='gray', markersize=8, markeredgecolor='black'),
            plt.Line2D([0], [0], marker='D', color='w', label='Shelter (Isolated)',
                    markerfacecolor='gray', markersize=7, markeredgecolor='black'),
            plt.Line2D([0], [0], color='red', linestyle='--', label='Cover Disk')
        ]
        ax.legend(handles=legend_elements, loc='upper right', fontsize=8)

        #ax.set_title("Equivalence Classes and Disk Covers", fontsize=12)
        ax.set_title(f"Equivalence Classes and Disk Covers{title_suffix}", fontsize=12)
        ax.set_xlabel("X [m]")
        ax.set_ylabel("Y [m]")
        ax.set_aspect('equal')
        ax.grid(True)
        plt.tight_layout()
        plt.savefig(output_path, bbox_inches='tight')
        plt.close()
        print(f"Saved: {output_path}")

    def summarize_equivalence_analysis(self,
                                        eq_csv_path="equivalence_assignment.csv",
                                        disk_csv_path="disk_cover_result.csv",
                                        output_summary_csv="equivalence_summary.csv"):
        """
        åŒå€¤é¡ã®çµ±è¨ˆãƒ»è¢«è¦†å††ã®çµ±è¨ˆãƒ»ä¸»åŒå€¤é¡å†…ã® shelter ã®ä¸­å¿ƒæ€§ãƒ»å®šå¸¸åˆ†å¸ƒæŒ‡æ¨™ã‚’é›†ç´„ã—ã¦ä¿å­˜
        """

        # åŒå€¤é¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = pd.read_csv(eq_csv_path)

        # 1. åŒå€¤é¡ã®å€‹æ•°
        num_equivalence_classes = df["equivalence_class"].nunique()

        # 2. ä¸»åŒå€¤é¡ã®ç‰¹å®š
        main_class = df["equivalence_class"].value_counts().idxmax()

        # 3. ä¸»åŒå€¤é¡ä»¥å¤–ã®è¦ç´ æ•°
        non_main_class_elements = df[df["equivalence_class"] != main_class].shape[0]

        # 4. è¢«è¦†å††ã®å€‹æ•°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        try:
            disk_df = pd.read_csv(disk_csv_path)
            num_cover_disks = 0 if disk_df.empty else disk_df.shape[0]
        except Exception:
            num_cover_disks = 0

        # 5. ä¸»åŒå€¤é¡ã«å±ã™ã‚‹ shelter ã®æŠ½å‡º
        shelters_main = df[(df["equivalence_class"] == main_class) & (df["type"] == "shelter")]

        eps = 1e-10
        max_in_all = df["in_degree"].max() + eps
        max_out_all = df["out_degree"].max() + eps
        max_undeg_all = df["undirected_degree"].max() + eps

        shelters_main = shelters_main.copy()
        shelters_main["in_degree_norm"] = shelters_main["in_degree"] / max_in_all
        shelters_main["out_degree_norm"] = shelters_main["out_degree"] / max_out_all
        shelters_main["undirected_degree_norm"] = shelters_main["undirected_degree"] / max_undeg_all

        # ä¸­å¿ƒæ€§ãƒ»å®šå¸¸åˆ†å¸ƒã®çµ±è¨ˆ
        mean_in_deg = shelters_main["in_degree"].mean()
        mean_out_deg = shelters_main["out_degree"].mean()
        mean_undeg = shelters_main["undirected_degree"].mean()

        max_in_deg = shelters_main["in_degree"].max()
        max_in_deg_id = shelters_main.loc[shelters_main["in_degree"].idxmax(), "id"]

        mean_stat = shelters_main["stationary"].mean()
        std_stat = shelters_main["stationary"].std()  # âœ… æ¨™æº–åå·®ã‚’è¿½åŠ 
        max_stat = shelters_main["stationary"].max()
        max_stat_id = shelters_main.loc[shelters_main["stationary"].idxmax(), "id"]

        mean_in_deg_norm = shelters_main["in_degree_norm"].mean()
        mean_out_deg_norm = shelters_main["out_degree_norm"].mean()
        mean_undeg_norm = shelters_main["undirected_degree_norm"].mean()

        max_in_deg_norm = shelters_main["in_degree_norm"].max()
        max_out_deg_norm = shelters_main["out_degree_norm"].max()
        max_undeg_norm = shelters_main["undirected_degree_norm"].max()

        max_in_deg_norm_id = shelters_main.loc[shelters_main["in_degree_norm"].idxmax(), "id"]
        max_out_deg_norm_id = shelters_main.loc[shelters_main["out_degree_norm"].idxmax(), "id"]
        max_undeg_norm_id = shelters_main.loc[shelters_main["undirected_degree_norm"].idxmax(), "id"]

        summary = {
            "num_equivalence_classes": num_equivalence_classes,
            "num_non_main_class_elements": non_main_class_elements,
            "num_cover_disks": num_cover_disks,

            "mean_in_degree_shelter_main": mean_in_deg,
            "mean_out_degree_shelter_main": mean_out_deg,
            "mean_undirected_degree_shelter_main": mean_undeg,
            "max_in_degree_shelter_main": max_in_deg,
            "max_in_degree_shelter_main_id": int(max_in_deg_id),

            "mean_in_degree_norm_shelter_main": mean_in_deg_norm,
            "mean_out_degree_norm_shelter_main": mean_out_deg_norm,
            "mean_undirected_degree_norm_shelter_main": mean_undeg_norm,
            "max_in_degree_norm_shelter_main": max_in_deg_norm,
            "max_in_degree_norm_shelter_main_id": int(max_in_deg_norm_id),
            "max_out_degree_norm_shelter_main": max_out_deg_norm,
            "max_out_degree_norm_shelter_main_id": int(max_out_deg_norm_id),
            "max_undirected_degree_norm_shelter_main": max_undeg_norm,
            "max_undirected_degree_norm_shelter_main_id": int(max_undeg_norm_id),

            "mean_stationary_shelter_main": mean_stat,
            "std_stationary_shelter_main": std_stat,  # âœ… è¿½åŠ 
            "max_stationary_shelter_main": max_stat,
            "max_stationary_shelter_main_id": int(max_stat_id)
        }

        pd.DataFrame([summary]).to_csv(output_summary_csv, index=False)
        print(f"âœ… Summary saved to: {output_summary_csv}")

        print("===== Equivalence Analysis Summary =====")
        print(f"ğŸ”¢ åŒå€¤é¡ã®å€‹æ•°: {num_equivalence_classes}")
        print(f"ğŸš· ä¸»åŒå€¤é¡ä»¥å¤–ã®è¦ç´ æ•°: {non_main_class_elements}")
        print(f"ğŸŸ¢ è¢«è¦†å††ã®å€‹æ•°: {num_cover_disks}")
        print("--- ä¸»åŒå€¤é¡å†… shelter ã®ä¸­å¿ƒæ€§æŒ‡æ¨™ï¼ˆéæ­£è¦åŒ–ï¼‰ ---")
        print(f"  ãƒ»å¹³å‡ in-degree : {mean_in_deg:.2f}")
        print(f"  ãƒ»å¹³å‡ out-degree: {mean_out_deg:.2f}")
        print(f"  ãƒ»å¹³å‡ undirected: {mean_undeg:.2f}")
        print(f"  ãƒ»æœ€å¤§ in-degree : {max_in_deg} (ID={int(max_in_deg_id)})")
        print("--- ä¸»åŒå€¤é¡å†… shelter ã®ä¸­å¿ƒæ€§æŒ‡æ¨™ï¼ˆæ­£è¦åŒ–ï¼‰ ---")
        print(f"  ãƒ»å¹³å‡ in-degree : {mean_in_deg_norm:.4f}")
        print(f"  ãƒ»å¹³å‡ out-degree: {mean_out_deg_norm:.4f}")
        print(f"  ãƒ»å¹³å‡ undirected: {mean_undeg_norm:.4f}")
        print(f"  ãƒ»æœ€å¤§ in-degree : {max_in_deg_norm:.4f} (ID={int(max_in_deg_norm_id)})")
        print(f"  ãƒ»æœ€å¤§ out-degree: {max_out_deg_norm:.4f} (ID={int(max_out_deg_norm_id)})")
        print(f"  ãƒ»æœ€å¤§ undirected: {max_undeg_norm:.4f} (ID={int(max_undeg_norm_id)})")
        print("--- ä¸»åŒå€¤é¡å†… shelter ã®å®šå¸¸åˆ†å¸ƒ ---")
        print(f"  ãƒ»å¹³å‡ stationary : {mean_stat:.6f}")
        print(f"  ãƒ»æ¨™æº–åå·® stationary: {std_stat:.6f}")  # âœ… å‡ºåŠ›ã«è¿½åŠ 
        print(f"  ãƒ»æœ€å¤§ stationary : {max_stat:.6f} (ID={int(max_stat_id)})")
        print("========================================")

        return summary


    '''
    def summarize_equivalence_analysis(self,
                                    eq_csv_path="equivalence_assignment.csv",
                                    disk_csv_path="disk_cover_result.csv",
                                    output_summary_csv="equivalence_summary.csv"):
        """
        åŒå€¤é¡ã®çµ±è¨ˆãƒ»è¢«è¦†å††ã®çµ±è¨ˆãƒ»ä¸»åŒå€¤é¡å†…ã® shelter ã®ä¸­å¿ƒæ€§ãƒ»å®šå¸¸åˆ†å¸ƒæŒ‡æ¨™ã‚’é›†ç´„ã—ã¦ä¿å­˜
        """

        # åŒå€¤é¡ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = pd.read_csv(eq_csv_path)

        # 1. åŒå€¤é¡ã®å€‹æ•°
        num_equivalence_classes = df["equivalence_class"].nunique()

        # 2. ä¸»åŒå€¤é¡ã®ç‰¹å®š
        main_class = df["equivalence_class"].value_counts().idxmax()

        # 3. ä¸»åŒå€¤é¡ä»¥å¤–ã®è¦ç´ æ•°
        non_main_class_elements = df[df["equivalence_class"] != main_class].shape[0]

        # 4. è¢«è¦†å††ã®å€‹æ•°ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        try:
            disk_df = pd.read_csv(disk_csv_path)
            if disk_df.empty:
                num_cover_disks = 0
            else:
                num_cover_disks = disk_df.shape[0]
        except Exception:
            num_cover_disks = 0

        # 5. shelterï¼ˆé¿é›£æ‰€ï¼‰ã§ä¸»åŒå€¤é¡ã«å±ã™ã‚‹ã‚‚ã®ã®ã¿æŠ½å‡º
        shelters_main = df[(df["equivalence_class"] == main_class) & (df["type"] == "shelter")]

        # æ­£è¦åŒ–ã®ãŸã‚ã«å…¨ä½“ã®æœ€å¤§å€¤å–å¾—ï¼ˆ0å‰²é˜²æ­¢ã®ãŸã‚epsã‚’åŠ ãˆã‚‹ï¼‰
        eps = 1e-10
        max_in_all = df["in_degree"].max() + eps
        max_out_all = df["out_degree"].max() + eps
        max_undeg_all = df["undirected_degree"].max() + eps

        # æ­£è¦åŒ–æ¸ˆã¿åˆ—ã®è¿½åŠ ï¼ˆä¸»åŒå€¤é¡ shelter ã®ã¿ã«å¯¾ã—ã¦ï¼‰
        shelters_main = shelters_main.copy()
        shelters_main["in_degree_norm"] = shelters_main["in_degree"] / max_in_all
        shelters_main["out_degree_norm"] = shelters_main["out_degree"] / max_out_all
        shelters_main["undirected_degree_norm"] = shelters_main["undirected_degree"] / max_undeg_all

        # å„ç¨®æŒ‡æ¨™ã®å¹³å‡ãƒ»æœ€å¤§ãƒ»ID
        mean_in_deg = shelters_main["in_degree"].mean()
        mean_out_deg = shelters_main["out_degree"].mean()
        mean_undeg = shelters_main["undirected_degree"].mean()

        max_in_deg = shelters_main["in_degree"].max()
        max_in_deg_id = shelters_main.loc[shelters_main["in_degree"].idxmax(), "id"]

        mean_stat = shelters_main["stationary"].mean()
        max_stat = shelters_main["stationary"].max()
        max_stat_id = shelters_main.loc[shelters_main["stationary"].idxmax(), "id"]

        # æ­£è¦åŒ–ä¸­å¿ƒæ€§æŒ‡æ¨™ï¼ˆå¹³å‡ãƒ»æœ€å¤§ï¼‰
        mean_in_deg_norm = shelters_main["in_degree_norm"].mean()
        mean_out_deg_norm = shelters_main["out_degree_norm"].mean()
        mean_undeg_norm = shelters_main["undirected_degree_norm"].mean()

        max_in_deg_norm = shelters_main["in_degree_norm"].max()
        max_out_deg_norm = shelters_main["out_degree_norm"].max()
        max_undeg_norm = shelters_main["undirected_degree_norm"].max()

        max_in_deg_norm_id = shelters_main.loc[shelters_main["in_degree_norm"].idxmax(), "id"]
        max_out_deg_norm_id = shelters_main.loc[shelters_main["out_degree_norm"].idxmax(), "id"]
        max_undeg_norm_id = shelters_main.loc[shelters_main["undirected_degree_norm"].idxmax(), "id"]

        # çµæœã¾ã¨ã‚
        summary = {
            "num_equivalence_classes": num_equivalence_classes,
            "num_non_main_class_elements": non_main_class_elements,
            "num_cover_disks": num_cover_disks,

            "mean_in_degree_shelter_main": mean_in_deg,
            "mean_out_degree_shelter_main": mean_out_deg,
            "mean_undirected_degree_shelter_main": mean_undeg,
            "max_in_degree_shelter_main": max_in_deg,
            "max_in_degree_shelter_main_id": int(max_in_deg_id),

            "mean_in_degree_norm_shelter_main": mean_in_deg_norm,
            "mean_out_degree_norm_shelter_main": mean_out_deg_norm,
            "mean_undirected_degree_norm_shelter_main": mean_undeg_norm,
            "max_in_degree_norm_shelter_main": max_in_deg_norm,
            "max_in_degree_norm_shelter_main_id": int(max_in_deg_norm_id),
            "max_out_degree_norm_shelter_main": max_out_deg_norm,
            "max_out_degree_norm_shelter_main_id": int(max_out_deg_norm_id),
            "max_undirected_degree_norm_shelter_main": max_undeg_norm,
            "max_undirected_degree_norm_shelter_main_id": int(max_undeg_norm_id),

            "mean_stationary_shelter_main": mean_stat,
            "max_stationary_shelter_main": max_stat,
            "max_stationary_shelter_main_id": int(max_stat_id)
        }

        # CSV å‡ºåŠ›
        pd.DataFrame([summary]).to_csv(output_summary_csv, index=False)
        print(f"âœ… Summary saved to: {output_summary_csv}")

        # ç”»é¢å‡ºåŠ›ï¼ˆè¦‹ã‚„ã™ãæ•´å½¢ï¼‰
        print("===== Equivalence Analysis Summary =====")
        print(f"ğŸ”¢ åŒå€¤é¡ã®å€‹æ•°: {num_equivalence_classes}")
        print(f"ğŸš· ä¸»åŒå€¤é¡ä»¥å¤–ã®è¦ç´ æ•°: {non_main_class_elements}")
        print(f"ğŸŸ¢ è¢«è¦†å††ã®å€‹æ•°: {num_cover_disks}")
        print("--- ä¸»åŒå€¤é¡å†… shelter ã®ä¸­å¿ƒæ€§æŒ‡æ¨™ï¼ˆéæ­£è¦åŒ–ï¼‰ ---")
        print(f"  ãƒ»å¹³å‡ in-degree : {mean_in_deg:.2f}")
        print(f"  ãƒ»å¹³å‡ out-degree: {mean_out_deg:.2f}")
        print(f"  ãƒ»å¹³å‡ undirected: {mean_undeg:.2f}")
        print(f"  ãƒ»æœ€å¤§ in-degree : {max_in_deg} (ID={int(max_in_deg_id)})")
        print("--- ä¸»åŒå€¤é¡å†… shelter ã®ä¸­å¿ƒæ€§æŒ‡æ¨™ï¼ˆæ­£è¦åŒ–ï¼‰ ---")
        print(f"  ãƒ»å¹³å‡ in-degree : {mean_in_deg_norm:.4f}")
        print(f"  ãƒ»å¹³å‡ out-degree: {mean_out_deg_norm:.4f}")
        print(f"  ãƒ»å¹³å‡ undirected: {mean_undeg_norm:.4f}")
        print(f"  ãƒ»æœ€å¤§ in-degree : {max_in_deg_norm:.4f} (ID={int(max_in_deg_norm_id)})")
        print(f"  ãƒ»æœ€å¤§ out-degree: {max_out_deg_norm:.4f} (ID={int(max_out_deg_norm_id)})")
        print(f"  ãƒ»æœ€å¤§ undirected: {max_undeg_norm:.4f} (ID={int(max_undeg_norm_id)})")
        print("--- ä¸»åŒå€¤é¡å†… shelter ã®å®šå¸¸åˆ†å¸ƒ ---")
        print(f"  ãƒ»å¹³å‡ stationary : {mean_stat:.6f}")
        print(f"  ãƒ»æœ€å¤§ stationary : {max_stat:.6f} (ID={int(max_stat_id)})")
        print("========================================")

        return summary
    '''

    def evaluate_model_score(self, summary_csv="equivalence_summary.csv",
                            lambda_=1.0, mu=1.0, nu=100.0, rm=0):
        """
        æŒ‡æ¨™ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€ç”»é¢è¡¨ç¤ºï¼‹è¿”å´

        è©•ä¾¡å¼: Î»Ã—åŒå€¤é¡ã®å€‹æ•° + Î¼Ã—è¢«è¦†å††ã®å€‹æ•° âˆ’ Î½Ã—æ­£è¦åŒ–in-degreeå¹³å‡
        """
        df = pd.read_csv(summary_csv)

        # å€¤å–å¾—
        n_eq = df.at[0, "num_equivalence_classes"]
        n_disks = df.at[0, "num_cover_disks"]
        mean_in_deg_norm = df.at[0, "mean_in_degree_norm_shelter_main"]

        # è©•ä¾¡å€¤è¨ˆç®—
        score = lambda_ * n_eq + mu * n_disks - nu * mean_in_deg_norm

        # è¡¨ç¤º
        print("===== Model Evaluation Score =====")
        print(f"ğŸ—» åˆ¶é™æ¨™é«˜ rm = {rm} [m]")
        print(f"ğŸ”¢ åŒå€¤é¡ã®å€‹æ•°: {n_eq}")
        print(f"ğŸŸ¢ è¢«è¦†å††ã®å€‹æ•°: {n_disks}")
        print(f"ğŸ“ˆ shelterã®æ­£è¦åŒ–in-degreeå¹³å‡: {mean_in_deg_norm:.4f}")
        print(f"ğŸ§® è©•ä¾¡å¼: Î»Ã—{n_eq} + Î¼Ã—{n_disks} âˆ’ Î½Ã—{mean_in_deg_norm:.4f}")
        print(f"â¡ï¸ è©•ä¾¡å€¤: {score:.4f}")
        print("===================================")

        return score
    
    def evaluate_model_score_std_stationary(self, summary_csv="equivalence_summary.csv",
                                            lambda_=1.0, mu=1.0, nu=100.0, rm=0):
        """
        æŒ‡æ¨™ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆã‚¹ã‚³ã‚¢ã¯å°ã•ã„ã»ã©è‰¯ã„ã¨ã™ã‚‹ï¼‰
        è©•ä¾¡å¼: Î»Ã—åŒå€¤é¡ã®å€‹æ•° + Î¼Ã—è¢«è¦†å††ã®å€‹æ•° + Î½Ã—å®šå¸¸åˆ†å¸ƒã®æ¨™æº–åå·®
        """
        df = pd.read_csv(summary_csv)

        n_eq = df.at[0, "num_equivalence_classes"]
        n_disks = df.at[0, "num_cover_disks"]
        std_stat = df.at[0, "std_stationary_shelter_main"]

        score = lambda_ * n_eq + mu * n_disks + nu * std_stat

        print("===== Model Evaluation Score (Smaller is Better) =====")
        print(f"ğŸ—» åˆ¶é™æ¨™é«˜ rm = {rm} [m]")
        print(f"ğŸ”¢ åŒå€¤é¡ã®å€‹æ•°: {n_eq}")
        print(f"ğŸŸ¢ è¢«è¦†å††ã®å€‹æ•°: {n_disks}")
        print(f"ğŸ“Š shelterã®å®šå¸¸åˆ†å¸ƒã®æ¨™æº–åå·®: {std_stat:.6f}")
        print(f"ğŸ§® è©•ä¾¡å¼: Î»Ã—{n_eq} + Î¼Ã—{n_disks} + Î½Ã—{std_stat:.6f}")
        print(f"â¡ï¸ è©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰: {score:.4f}")
        print("===================================")

        return score

    def run_elevation_sweep_analysis_std_stationary(self, rm_min=0, rm_max=15, lambda_=1.0, mu=1.0, nu=10.0):
        """
        æ¨™é«˜åˆ¶é™ rm ã‚’ rm_min ã‹ã‚‰ rm_max ã¾ã§1måˆ»ã¿ã§å¤‰åŒ–ã•ã›ãªãŒã‚‰ã€
        å„ rm ã«å¯¾å¿œã™ã‚‹ç½å®³ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ã™ã‚‹ã€‚
        
        å®Ÿè¡Œå†…å®¹ï¼š
            â‘  rm ä»¥ä¸‹ã®åœ°å½¢å‰²åˆãƒãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
            â‘¡ å±é™ºãªçµŒè·¯ã‚’é™¤ã„ãŸé‡åŠ›ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šæ¨ç§»ç¢ºç‡è¡Œåˆ—ã‚’æ§‹ç¯‰
            â‘¢ åŒå€¤é¡ã‚’åˆ†é¡ã—ã€å®šå¸¸åˆ†å¸ƒã¨ä¸­å¿ƒæ€§ã‚’å«ã‚ãŸåˆ†æã‚’å®Ÿæ–½
            â‘£ ä¸»åŒå€¤é¡ã«å±ã•ãªã„æ‹ ç‚¹ã‚’è²ªæ¬²æ³•ã§è¢«è¦†ï¼ˆdisk coverï¼‰
            â‘¤ çµ±è¨ˆã‚’ã¾ã¨ã‚ãŸ summary CSV ã‚’ä¿å­˜
            â‘¥ å®šå¸¸åˆ†å¸ƒã®æ¨™æº–åå·®ã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰
            â‘¦ çµæœã‚’CSVã«ä¿å­˜
            â‘§ å„è©•ä¾¡æŒ‡æ¨™ã®ã‚°ãƒ©ãƒ•ã‚’æç”»ãƒ»ä¿å­˜
            â‘¨ ã‚¹ã‚³ã‚¢ã®ç´¯ç©å’Œã‚’ã‚°ãƒ©ãƒ•æç”»ãƒ»ä¿å­˜
        """
        results = []

        for rm in range(rm_min, rm_max + 1):
            print(f"\n=== æ¨™é«˜ {rm}m ã®è§£æã‚’é–‹å§‹ ===")
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«åæº–å‚™
                lowland_csv = f"lowland_fraction_matrix_rm{rm}.csv"
                transition_csv = f"transition_matrix_rm{rm}.csv"
                eq_assign_csv = f"equivalence_assignment_rm{rm}.csv"
                eq_summary_csv = f"equivalence_summary_rm{rm}.csv"
                disk_csv = f"disk_cover_result_rm{rm}.csv"

                # â‘  æ¨™é«˜rmä»¥ä¸‹ã®åœ°å½¢å‰²åˆãƒãƒˆãƒªã‚¯ã‚¹ã‚’ä½œæˆ
                self.compute_lowland_fraction_matrix(rm=rm, output_csv=lowland_csv)

                # â‘¡ å±é™ºçµŒè·¯é™¤å»æ¸ˆã¿ã®æ¨ç§»ç¢ºç‡è¡Œåˆ—ã‚’æ§‹ç¯‰
                self.compute_transition_matrix_from_csv(
                    lowland_matrix_csv=lowland_csv,
                    rp=0.5,  # 50%ä»¥ä¸ŠãŒä½åœ°ãªã‚‰é™¤å¤–
                    output_csv=transition_csv
                )

                # â‘¢ åŒå€¤é¡ã¨ä¸­å¿ƒæ€§ã€å®šå¸¸åˆ†å¸ƒã‚’è§£æãƒ»CSVå‡ºåŠ›
                self.analyze_equivalence_classes(
                    transition_matrix_csv=transition_csv,
                    location_csv="locations.csv",
                    output_node_csv=eq_assign_csv,
                    output_summary_csv=eq_summary_csv,
                    threshold=1e-3
                )

                # åŒå€¤é¡ã®ç¢ºèªè¡¨ç¤º
                df_eq = pd.read_csv(eq_assign_csv)
                print(f"ğŸ“Š [rm={rm}] Unique equivalence classes:", df_eq["equivalence_class"].unique())
                print(f"ğŸ“Š [rm={rm}] Class counts:\n", df_eq["equivalence_class"].value_counts())

                # â‘£ ä¸»åŒå€¤é¡å¤–ãƒãƒ¼ãƒ‰ã‚’è¢«è¦†ã™ã‚‹disk coverå®Ÿè¡Œ
                self.compute_disk_cover(
                    input_node_csv=eq_assign_csv,
                    output_csv=disk_csv
                )

                # è¢«è¦†å††ãŒãªã‘ã‚Œã°ã€ç­‰é«˜ç·šä»˜ããƒãƒƒãƒ—ã‚’ä¿å­˜
                if not os.path.exists(disk_csv):
                    eq_map_output = f"equivalence_map_rm{rm}.png"
                    self.visualize_equivalence_classes(
                        eq_csv_path=eq_assign_csv,
                        output_path=eq_map_output,
                        figsize=(10, 10),
                        dpi=300
                    )
                    print(f"âœ… è¢«è¦†å††ãªã—: Saved {eq_map_output}")

                # â‘¤ çµ±è¨ˆ summary ã‚’ä¿å­˜ï¼ˆå®šå¸¸åˆ†å¸ƒæ¨™æº–åå·®å«ã‚€ï¼‰
                summary = self.summarize_equivalence_analysis(
                    eq_csv_path=eq_assign_csv,
                    disk_csv_path=disk_csv,
                    output_summary_csv=eq_summary_csv
                )

                # â‘¥ è©•ä¾¡é–¢æ•°ï¼ˆå®šå¸¸åˆ†å¸ƒã®æ¨™æº–åå·®ã‚’ä½¿ç”¨ï¼‰
                score = self.evaluate_model_score_std_stationary(
                    summary_csv=eq_summary_csv,
                    lambda_=lambda_,
                    mu=mu,
                    nu=nu,
                    rm=rm
                )

                # çµæœã« rm ã¨ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ 
                summary["elevation"] = rm
                summary["score"] = score
                results.append(summary)

                # â‘¦ å¯è¦–åŒ–ï¼šdiskã¨åŒå€¤é¡ã‚’é‡ã­ãŸãƒãƒƒãƒ—å‡ºåŠ›
                self.visualize_equivalence_and_disks(
                    eq_csv_path=eq_assign_csv,
                    disk_csv_path=disk_csv,
                    output_path="equivalence_with_disks.png",
                    rm=rm,
                    figsize=(10, 10),
                    dpi=300
                )

            except Exception as e:
                print(f"âš ï¸ æ¨™é«˜ {rm}m ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # â‘§ å…¨çµæœã‚’CSVä¿å­˜
        df_results = pd.DataFrame(results).sort_values("elevation")
        df_results["score_cumsum"] = df_results["score"].cumsum()
        df_results.to_csv("evaluation_over_elevation_std_stationary.csv", index=False)
        print("âœ… å…¨ã¦ã®çµæœã‚’ evaluation_over_elevation_std_stationary.csv ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

        # â‘¨ ã‚°ãƒ©ãƒ•æç”»ï¼šå„æŒ‡æ¨™ã¨ã‚¹ã‚³ã‚¢ã®æ¨ç§»
        plt.figure(figsize=(10, 6))
        score_equiv = df_results["num_equivalence_classes"] * lambda_
        score_cover = df_results["num_cover_disks"] * mu
        score_std_stat = df_results["std_stationary_shelter_main"] * nu

        plt.plot(df_results["elevation"], score_equiv, marker='o', label=f"Equivalence Classes Ã— Î» ({lambda_})")
        plt.plot(df_results["elevation"], score_cover, marker='s', label=f"Cover Disks Ã— Î¼ ({mu})")
        plt.plot(df_results["elevation"], score_std_stat, marker='^', label=f"Std(Stationary) Ã— Î½ ({nu})")
        plt.plot(df_results["elevation"], df_results["score"], marker='x', linestyle='--', color='black', label="Total Score")

        plt.xlabel("Elevation Threshold [m]")
        plt.ylabel("Weighted Metric Value")
        plt.title(f"Evaluation Metrics vs Elevation (using Std of Stationary, Î»={lambda_}, Î¼={mu}, Î½={nu})")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig("metrics_vs_elevation_std_stationary.png", dpi=300)
        plt.close()

        # ç´¯ç©ã‚¹ã‚³ã‚¢ã®ãƒ—ãƒ­ãƒƒãƒˆ
        plt.figure(figsize=(10, 6))
        plt.plot(df_results["elevation"], df_results["score_cumsum"], marker='o', color='black')
        plt.xlabel("Elevation Threshold [m]")
        plt.ylabel("Cumulative Score")
        plt.title("Cumulative Evaluation Score (Std Stationary Version)")
        plt.grid(True)
        plt.savefig("cumulative_score_vs_elevation_std_stationary.png", dpi=300)
        plt.close()

        return df_results

    def run_elevation_sweep_analysis(self, rm_min=0, rm_max=15, lambda_=1.0, mu=1.0, nu=10.0):
        """
        æ¨™é«˜åˆ¶é™ rm ã‚’ rm_min ã‹ã‚‰ rm_max ã¾ã§1måˆ»ã¿ã§å¤‰åŒ–ã•ã›ãªãŒã‚‰ã€
        å„ rm ã«å¯¾å¿œã™ã‚‹ç½å®³ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ã—ã€æ¨ç§»ç¢ºç‡ã®æ§‹ç¯‰ â†’ åŒå€¤é¡è§£æ â†’ è¢«è¦†å††è©•ä¾¡ â†’
        æŒ‡æ¨™ã‚µãƒãƒªã¨ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€CSVã¨ã‚°ãƒ©ãƒ•ã§çµæœã‚’ä¿å­˜ã™ã‚‹ã€‚
        """
        results = []

        for rm in range(rm_min, rm_max + 1):
            print(f"\n=== æ¨™é«˜ {rm}m ã®è§£æã‚’é–‹å§‹ ===")
            try:
                # å„ rm ã«å¿œã˜ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”¨æ„
                lowland_csv = f"lowland_fraction_matrix_rm{rm}.csv"
                transition_csv = f"transition_matrix_rm{rm}.csv"
                eq_assign_csv = f"equivalence_assignment_rm{rm}.csv"
                eq_summary_csv = f"equivalence_summary_rm{rm}.csv"
                disk_csv = f"disk_cover_result_rm{rm}.csv"

                # â‘  å„æ‹ ç‚¹ãƒšã‚¢é–“ã®ã€Œrmä»¥ä¸‹ã®æ¨™é«˜ã‚’é€šã‚‹å‰²åˆã€è¡Œåˆ—ã‚’ä½œæˆãƒ»ä¿å­˜
                self.compute_lowland_fraction_matrix(rm=rm, output_csv=lowland_csv)

                # â‘¡ ä½åœ°å‰²åˆã«åŸºã¥ã„ã¦ã€Œå±é™ºãªçµŒè·¯ã‚’é™¤å¤–ã€ã—ã€é‡åŠ›ãƒ¢ãƒ‡ãƒ«ã§æ¨ç§»ç¢ºç‡è¡Œåˆ—ã‚’ä½œæˆãƒ»ä¿å­˜
                self.compute_transition_matrix_from_csv(
                    lowland_matrix_csv=lowland_csv,
                    rp=0.5,  # å±é™ºãªçµŒè·¯ï¼ˆä½åœ°å‰²åˆ â‰¥ 50%ï¼‰ã¯é™¤å¤–
                    output_csv=transition_csv
                )

                # â‘¢ ä½œæˆã—ãŸæ¨ç§»ç¢ºç‡è¡Œåˆ—ã‹ã‚‰åŒå€¤é¡ã‚„ä¸­å¿ƒæ€§ã‚’è§£æã—ã€ãƒãƒ¼ãƒ‰åˆ†é¡ã¨ã‚µãƒãƒªã‚’CSVå‡ºåŠ›
                self.analyze_equivalence_classes(
                    transition_matrix_csv=transition_csv,
                    location_csv="locations.csv",
                    output_node_csv=eq_assign_csv,
                    output_summary_csv=eq_summary_csv,
                    threshold=1e-3
                )

                # âœ… åŒå€¤é¡CSVã‚’èª­ã¿è¾¼ã‚“ã§ã‚¯ãƒ©ã‚¹ã®çŠ¶æ³ã‚’ç¢ºèª
                df_eq = pd.read_csv(eq_assign_csv)
                print(f"ğŸ“Š [rm={rm}] Unique equivalence classes:", df_eq["equivalence_class"].unique())
                print(f"ğŸ“Š [rm={rm}] Class counts:\n", df_eq["equivalence_class"].value_counts())

                # â‘£ ä¸»åŒå€¤é¡ã«å±ã•ãªã„æ‹ ç‚¹ã‚’è²ªæ¬²æ³•ã§è¢«è¦†ã—ã€è¢«è¦†å††ã®æƒ…å ±ã‚’ä¿å­˜
                self.compute_disk_cover(
                    input_node_csv=eq_assign_csv,
                    output_csv=disk_csv
                )
                # âœ… è¢«è¦†å††ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ â†’ è¢«è¦†å††ãªã—ã®ç­‰é«˜ç·šä»˜ãå¯è¦–åŒ–å›³ã‚’å‡ºåŠ›
                if not os.path.exists(disk_csv):
                    eq_map_output = f"equivalence_map_rm{rm}.png"
                    self.visualize_equivalence_classes(
                        eq_csv_path=eq_assign_csv,
                        output_path=eq_map_output,
                        figsize=(10, 10),
                        dpi=300
                    )
                    print(f"âœ… è¢«è¦†å††ãªã—: Saved {eq_map_output}")

                # â‘¤ å„ç¨®çµ±è¨ˆï¼ˆåŒå€¤é¡æ•°ãƒ»è¢«è¦†å††æ•°ãƒ»shelterã®ä¸­å¿ƒæ€§ãƒ»å®šå¸¸åˆ†å¸ƒï¼‰ã‚’ã¾ã¨ã‚ãŸã‚µãƒãƒªã‚’CSVå‡ºåŠ›
                summary = self.summarize_equivalence_analysis(
                    eq_csv_path=eq_assign_csv,
                    disk_csv_path=disk_csv,
                    output_summary_csv=eq_summary_csv
                )

                # âœ… è¢«è¦†å††ã‚’é‡ã­ãŸå¯è¦–åŒ–å›³ã‚’ä¿å­˜ï¼ˆrmã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«åãŒå¤‰ã‚ã‚‹ï¼‰
                self.visualize_equivalence_and_disks(
                    eq_csv_path=eq_assign_csv,
                    disk_csv_path=disk_csv,
                    output_path="equivalence_with_disks.png",  # rmã«å¿œã˜ã¦è‡ªå‹•ã§ suffix ãŒä»˜ã
                    rm=rm,
                    figsize=(10, 10),
                    dpi=300
                )

                # â‘¥ è©•ä¾¡å¼ã«åŸºã¥ãã€ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                score = self.evaluate_model_score(
                    summary_csv=eq_summary_csv,
                    lambda_=lambda_,
                    mu=mu,
                    nu=nu,
                    rm=rm
                )

                # çµæœã‚’è¨˜éŒ²ï¼ˆæ¨™é«˜ã¨ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼‰
                summary["elevation"] = rm
                summary["score"] = score
                results.append(summary)

            except Exception as e:
                print(f"âš ï¸ æ¨™é«˜ {rm}m ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # â‘¦ ã™ã¹ã¦ã® rm ã«å¯¾ã™ã‚‹çµæœã‚’DataFrameã«ã¾ã¨ã‚ã¦CSVä¿å­˜
        df_results = pd.DataFrame(results).sort_values("elevation")
        df_results["score_cumsum"] = df_results["score"].cumsum()
        df_results.to_csv("evaluation_over_elevation.csv", index=False)
        print("âœ… å…¨ã¦ã®çµæœã‚’ evaluation_over_elevation.csv ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

        # â‘§ æ¨™é«˜ã”ã¨ã®æŒ‡æ¨™ã®æ¨ç§»ã‚’æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã«ã—ã¦ä¿å­˜ï¼ˆé‡ã¿ä»˜ãã§è¡¨ç¤ºï¼‰

        plt.figure(figsize=(10, 6))

        # é‡ã¿ã‚’ã‹ã‘ãŸæŒ‡æ¨™
        score_equiv = df_results["num_equivalence_classes"] * lambda_
        score_cover = df_results["num_cover_disks"] * mu
        score_in_deg = df_results["mean_in_degree_norm_shelter_main"] * nu

        plt.plot(df_results["elevation"], score_equiv, marker='o', label=f"Equivalence Classes Ã— Î» ({lambda_})")
        plt.plot(df_results["elevation"], score_cover, marker='s', label=f"Cover Disks Ã— Î¼ ({mu})")
        plt.plot(df_results["elevation"], score_in_deg, marker='^', label=f"Mean In-degree Ã— Î½ ({nu})")
        plt.plot(df_results["elevation"], df_results["score"], marker='x', linestyle='--', color='black', label="Total Score")

        plt.xlabel("Elevation Threshold [m]")
        plt.ylabel("Weighted Metric Value")
        plt.title(f"Evaluation Metrics vs Elevation (Î»={lambda_}, Î¼={mu}, Î½={nu})")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig("metrics_vs_elevation.png", dpi=300)
        plt.close()

        # â‘¨ è©•ä¾¡ã‚¹ã‚³ã‚¢ã®ç´¯ç©å’Œã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        plt.figure(figsize=(10, 6))
        plt.plot(df_results["elevation"], df_results["score_cumsum"], marker='o', color='black')
        plt.xlabel("Elevation Threshold [m]")
        plt.ylabel("Cumulative Score")
        plt.title("Cumulative Evaluation Score")
        plt.grid(True)
        plt.savefig("cumulative_score_vs_elevation.png", dpi=300)
        plt.close()

        return df_results
    

if __name__ == "__main__":

    sim = DisasterRiskAssessment(num_people=300, num_shelters=30, mean_z=15, std_z=5, max_z=100)

    # åœ°å½¢ã¨æ‹ ç‚¹ã®åˆæœŸç”Ÿæˆï¼ˆ1å›ã ã‘ã§OKï¼‰
    sim.generate_terrain(
        resolution=300,
        num_peaks=5,
        mean_height=15,
        std_height=5,
        min_spread=1500,
        max_spread=3000
    )
    sim.save_terrain_3d_plot("terrain_3d_map.png") # âœ… åœ°å½¢ã®3Dã‚µãƒ¼ãƒ•ã‚§ã‚¹å›³ï¼ˆ1å›ã ã‘ã§OKï¼‰
    sim.generate_locations_from_terrain() # æ‹ ç‚¹ã®ç”Ÿæˆï¼ˆåœ°å½¢ã«ä¾å­˜ï¼‰
    sim.show_area(filename="evacuation_map.png", figsize=(5, 5), dpi=200) # âœ… æ‹ ç‚¹ã®å¹³é¢é…ç½®å›³ï¼ˆ1å›ã ã‘ã§OKï¼‰
    sim.save_to_csv("locations.csv", person_popularity=1, shelter_popularity=30) # æ‹ ç‚¹æƒ…å ±ã®ä¿å­˜ï¼ˆpopularityä»˜ãï¼‰

    # elevation sweep ã®å®Ÿè¡Œï¼ˆè©•ä¾¡ã¨å¯è¦–åŒ–ã‚’å«ã‚€ï¼‰
    #sim.run_elevation_sweep_analysis(rm_min=0, rm_max=15, lambda_=1.0, mu=1.0, nu=10.0)
    #sim.run_elevation_sweep_analysis_std_stationary(rm_min=0, rm_max=15, lambda_=1.0, mu=1.0, nu=1000.0)
    
    # âœ… Î» ã®è‡ªå‹•è¨ˆç®—ï¼ˆ100 / å…¨æ‹ ç‚¹æ•°ï¼‰
    total_nodes = sim.num_people + sim.num_shelters
    lambda_auto = 100.0 / total_nodes

    # elevation sweep å®Ÿè¡Œï¼ˆè©•ä¾¡é–¢æ•°ã« Î» ã‚’è‡ªå‹•è¨­å®šï¼‰(2025/07/23)
    sim.run_elevation_sweep_analysis_std_stationary(
        rm_min=0, rm_max=15,
        lambda_=lambda_auto,
        mu=1.0,
        nu=1000.0
    )





